# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

import math
from dataclasses import dataclass
from typing import Literal

import torch
from torch import Tensor
from whetstone.core.model import Model
from whetstone.core.module import BaseModule, BaseState, ModuleRegistry, StatefulModule
from whetstone.utils import JSONType


class DummyModelState(BaseState):
    """State for the DummyModel."""
    some_expensive_model: Literal["nothing"] = "nothing"


@ModuleRegistry.register
class DummyModel(Model, StatefulModule[DummyModelState]):
    """A dummy model implementation for testing and demonstration.

    Generates repetitive text based on its state and implements basic
    byte-level tokenization/detokenization.
    """
    model_name: str
    """A name for this dummy model instance."""

    def generate(self, x: str, max_tokens: int = 100, **kwargs) -> str:
        """Generates a fixed, repetitive string up to max_tokens bytes.

        Ignores the input prompt `x` and any additional `kwargs`.
        Uses the `some_expensive_model` string from the state for repetition.

        Args:
            x: Input prompt (ignored).
            max_tokens: The maximum number of bytes (not tokens) to generate.
            **kwargs: Additional arguments (ignored).

        Returns:
            A string generated by repeating the state's base string, truncated to `max_tokens` bytes.
        """
        base_string = self.state.some_expensive_model
        base_bytes = base_string.encode('utf-8')
        
        if not base_bytes: # Avoid division by zero if base_string is empty
            base_bytes = b"a"

        num_repeats = math.ceil(max_tokens / len(base_bytes))
        repeated_bytes = (base_bytes * num_repeats)[:max_tokens]
        
        return repeated_bytes.decode('utf-8', errors='replace')

    def tokenize(self, x: str, **kwargs) -> Tensor:
        """Tokenizes a string into its constituent bytes.

        Each byte is treated as a token ID.

        Args:
            x: The input string.
            **kwargs: Additional arguments (ignored).

        Returns:
            A 1D LongTensor containing the byte values (0-255) of the UTF-8 encoded string.
        """
        byte_representation = list(x.encode('utf-8'))
        # Shape: (num_bytes,)
        return torch.tensor(byte_representation, dtype=torch.long)

    def detokenize(self, x: Tensor) -> str | list[str]:
        """Detokenizes a tensor of byte values back into a string or list of strings.

        Treats each value in the tensor as a byte.
        Handles both 1D (single sequence) and 2D (batch) tensors.
        Invalid bytes are replaced using the 'replace' error handler.

        Args:
            x: A LongTensor containing byte values (expected range 0-255).

        Returns:
            The decoded UTF-8 string, or a list of strings if the input was 2D.
        """
        def decode(x):
            byte_list = x.tolist()
            # Ensure all values are valid byte values
            byte_list = [b & 0xFF for b in byte_list] 
            return bytes(byte_list).decode('utf-8', errors='replace')

        if x.ndim == 2 and x.shape[1] > 1:
            return [decode(x[i]) for i in range(x.shape[0])]
        elif x.ndim == 1:
            return decode(x)
        
        

    def vocab_size(self) -> int:
        """Returns the size of the vocabulary (number of possible byte values)."""
        return 256
    
    def render_conversation_template(self, conversation: list[dict[str, str]]) -> str:
        """Renders a conversation into a simple role: content format.

        Args:
            conversation: A list of message dictionaries with 'role' and 'content'.

        Returns:
            A multi-line string with each message on a new line, formatted as "role: content".
        """
        return "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation])
    
    
